in the case of normal distribution data , the three sigma rule means that roughly 1 in 22 observations will differ by twice the standard deviation or more from the mean , and 1 in 370 will deviate by three times the standard deviation if the sample size is only 100 , however , just three such outliers are already reason for concern , being more than 11 times the expected number a physical apparatus for taking measurements may have suffered a transient malfunction there may have been an error in data transmission or transcription outliers arise due to changes in system behaviour , fraudulent behaviour , human error , instrument error or simply through natural deviations in populations a sample may have been contaminated with elements from outside the population being examined additionally , the pathological appearance of outliers of a certain form appears in a variety of datasets , indicating that the causative mechanism for the data might differ at the extreme end ( king effect ) there is no rigid mathematical definition of what constitutes an outlier ; determining whether or not an observation is an outlier is ultimately a subjective exercise it is proposed to determine in a series of m observations the limit of error , beyond which all observations involving so great an error may be rejected , provided there are as many as n such observations john tukey proposed this test , where k 1.5 indicates an '' outlier '' , and k 3 indicates data that is '' far out '' even when a normal distribution model is appropriate to the data being analyzed , outliers are expected for large sample sizes and should not automatically be discarded if that is the case in regression problems , an alternative approach may be to only exclude points which exhibit a large degree of influence on the estimated coefficients , using a measure such as cook 's distance when outliers occur , this intersection could be empty , and we should relax a small number of the sets x i ( as small as possible ) in order to avoid any inconsistency 