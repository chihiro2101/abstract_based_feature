in the case of normal distribution data , the three sigma rule means that roughly 1 in 22 observations will differ by twice the standard deviation or more from the mean , and 1 in 370 will deviate by three times the standard deviation in a sample of 1000 observations , the presence of up to five observations deviating from the mean by more than three times the standard deviation is within the range of what can be expected , being less than twice the expected number and hence within 1 standard deviation of the expected number – see poisson distribution – and not indicate an anomaly if the sample size is only 100 , however , just three such outliers are already reason for concern , being more than 11 times the expected number thus if one takes a normal distribution with cutoff 3 standard deviations from the mean , p is approximately 0.3 % , and thus for 1000 trials one can approximate the number of samples whose deviation exceeds 3 sigmas by a poisson distribution with λ 3 outliers arise due to changes in system behaviour , fraudulent behaviour , human error , instrument error or simply through natural deviations in populations additionally , the pathological appearance of outliers of a certain form appears in a variety of datasets , indicating that the causative mechanism for the data might differ at the extreme end ( king effect ) it is proposed to determine in a series of m observations the limit of error , beyond which all observations involving so great an error may be rejected , provided there are as many as n such observations in various domains such as , but not limited to , statistics , signal processing , finance , econometrics , manufacturing , networking and data mining , the task of anomaly detection may take other approaches the strength of this method lies in the fact that it takes into account a data set 's standard deviation , average and provides a statistically determined rejection zone ; thus providing an objective method to determine if a data point is an outlier thirdly , a rejection region is determined using the formula : : \text p ( y x , g_j ( t , \alpha ) ) where g_j ( t , \alpha ) is the hypothesis induced by learning algorithm g_j trained on training set with hyperparameters \alpha even when a normal distribution model is appropriate to the data being analyzed , outliers are expected for large sample sizes and should not automatically be discarded if that is the case an outlier resulting from an instrument reading error may be excluded but it is desirable that the reading is at least verified from mathworld -- a wolfram web resource the sample variance increases with the sample size , the sample mean fails to converge as the sample size increases , and outliers are expected at far larger rates than for a normal distribution 