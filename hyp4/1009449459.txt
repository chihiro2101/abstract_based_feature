successful statistical practice is based on focused problem definition because there is very rarely enough time or money to gather information from everyone or everything in a population , the goal becomes finding a representative sample ( or subset ) of that population although the population of interest often consists of physical objects , sometimes it is necessary to sample over time , space , or some combination of these dimensions for instance , an investigation of supermarket staffing could examine checkout line length at various times , or a study on endangered penguins might aim to understand their usage of various hunting grounds over time this situation often arises when seeking knowledge about the cause system of which the observed population is an outcome for example , a researcher might study the success rate of a new 'quit smoking ' program on a test group of 100 patients , in order to predict the effects of the program if it were made available nationwide often there is large but not complete overlap between these two groups due to frame issues etc time spent in making the sampled population and population of concern precise is often well spent , because it raises many issues , ambiguities and questions that would otherwise have been overlooked at this stage in the most straightforward case , such as the sampling of a batch of material from production ( acceptance sampling by lots ) , it would be most desirable to identify and measure every single item in the population and to include any one of them in our sample however , in the more general case this is not usually possible or practical these imprecise populations are not amenable to sampling in any of the ways below and to which we could apply statistical theory as a remedy , we seek a sampling frame which has the property that we can identify every single element and include any in our sample the most straightforward type of frame is a list of elements of the population ( preferably the entire population ) with appropriate contact information a probability sample is a sample in which every unit in the population has a chance ( greater than zero ) of being selected in the sample , and this probability can be accurately determined ( for example , we can allocate each person a random number , generated from a uniform distribution between 0 and 1 , and select the person with the highest number in each household ) to reflect this , when we come to such a household , we would count the selected person 's income twice towards the total ( the person who is selected from that household can be loosely viewed as also representing the person who is n't selected when every element in the population does have the same probability of selection , this is known as an 'equal probability of selection ' ( eps ) design probability sampling includes : simple random sample , systematic sampling , stratified sampling , probability proportional to size sampling , and cluster or multistage sampling these various ways of probability sampling have two things in common : every element has a known nonzero probability of being sampled and involves random selection at some point nonprobability sampling is any sampling method where some elements of the population have no chance of selection ( these are sometimes referred to as 'out of coverage'/'undercovered ' ) , or where the probability of selection ca n't be accurately determined hence , because the selection of elements is nonrandom , nonprobability sampling does not allow the estimation of sampling errors information about the relationship between sample and population is limited , making it difficult to extrapolate from the sample to the population in any household with more than one occupant , this is a nonprobability sample , because some people are more likely to answer the door ( e.g nonprobability sampling methods include convenience sampling , quota sampling and purposive sampling in addition , nonresponse effects may turn any probability design into a nonprobability design if the characteristics of nonresponse are not well understood , since nonresponse effectively modifies each element 's probability of being sampled within any of the types of frames identified above , a variety of sampling methods can be employed , individually or in combination in particular , the variance between individual results within the sample is a good indicator of variance in the overall population , which makes it relatively easy to estimate the accuracy of results simple random sampling can be vulnerable to sampling error because the randomness of the selection may result in a sample that does n't reflect the makeup of the population systematic and stratified techniques attempt to overcome this problem by '' using information about the population '' to choose a more '' representative '' sample also , simple random sampling can be cumbersome and tedious when sampling from a large target population simple random sampling can not accommodate the needs of researchers in this situation , because it does not provide subsamples of the population , and other sampling strategies , such as stratified sampling , can be used instead in this case , k ( population size/sample size ) it is important that the starting point is not automatically the first in the list , but is instead randomly chosen from within the first to the kth element in the list a simple example would be to select every 10th name from the telephone directory ( an 'every 10th ' sample , also referred to as 'sampling with a skip of 10 ' ) it is easy to implement and the stratification induced can make it efficient , if the variable by which the list is ordered is correlated with the variable of interest for example , suppose we wish to sample people from a long street that starts in a poor area ( house no a simple random selection of addresses from this street could easily end up with too many from the high end and too few from the low end ( or vice versa ) , leading to an unrepresentative sample however , systematic sampling is especially vulnerable to periodicities in the list if periodicity is present and the period is a multiple or factor of the interval used , the sample is especially likely to be unrepresentative of the overall population , making the scheme less accurate than simple random sampling another drawback of systematic sampling is that even in scenarios where it is more accurate than srs , its theoretical properties make it difficult to quantify that accuracy ) as described above , systematic sampling is an eps method , because all elements have the same probability of selection ( in the example given , one in ten ) systematic sampling can also be adapted to a non-eps approach ; for an example , see discussion of pps samples below when the population embraces a number of distinct categories , the frame can be organized by these categories into separate '' strata '' each stratum is then sampled as an independent sub-population , out of which individual elements can be randomly selected first , dividing the population into distinct , independent strata can enable researchers to draw inferences about specific subgroups that may be lost in a more generalized random sample second , utilizing a stratified sampling method can lead to more efficient statistical estimates ( provided that strata are selected based upon relevance to the criterion in question , instead of availability of the samples ) there are , however , some potential drawbacks to using stratified sampling ; a stratified sampling approach is most effective when three conditions are met : variability within strata are minimized variability between strata are maximized the variables upon which the population is stratified are strongly correlated with the desired dependent variable although the method is susceptible to the pitfalls of post hoc approaches , it can provide several benefits in the right situation in choice-based sampling , the data are stratified on the target and a sample is taken from each stratum so that the rare target class will be more represented in the sample in some cases the sample designer has access to an '' auxiliary variable '' or '' size measure '' , believed to be correlated to the variable of interest , for each element in the population another option is probability proportional to size ( 'pps ' ) sampling , in which the selection probability for each element is set to be proportional to its size measure , up to a maximum of 1 systematic sampling theory can be used to create a probability proportionate to size sample in this case , the sampling ratio of classes is selected so that the worst case classifier error over all the possible population statistics for class prior probabilities , would be the best random sampling by using lots is an old idea , mentioned several times in the bible 